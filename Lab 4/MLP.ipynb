{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "INIhhQnriyFt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building A Multilayer Perceptron (from Scratch)\n",
        "\n",
        "### by Mitchell Thomas"
      ]
    },
    {
      "metadata": {
        "id": "BHGsD20nbTQ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load in necessary packages\n",
        "\n",
        "These are the packages that I will be using within this Neural Network model."
      ]
    },
    {
      "metadata": {
        "id": "WAqdG1F8i2vc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1mthY5iqjZlf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Establish and modify input parameters\n",
        "\n",
        "Modify the parameters to correlate with the input dataset."
      ]
    },
    {
      "metadata": {
        "id": "OstMnuQdjfJC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Dataset\n",
        "iris = sns.load_dataset('iris')\n",
        "iris.head()\n",
        "\n",
        "# Split Dataset up into feature and label vectors\n",
        "X = iris.iloc[:,0:3]\n",
        "y = iris.iloc[:,4]\n",
        "\n",
        "# learning rate and initial lambda initialization\n",
        "lr = .0001\n",
        "init_lam = .01\n",
        "\n",
        "# modify the epochNum, or number of iterations that our network runs\n",
        "epochNum = 10000\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmSqEhzfaeNy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing\n",
        "\n",
        "### Pre-process the data so that it can smoothly run through the Multilayer Perceptron.\n",
        "\n",
        "### Process includes:\n",
        "\n",
        "\n",
        "1.   Establishing numerical values to the label vector (y) using Sklearn's One Hot Encoder.  The One Hot encoding provides a binarization of data so that the neural network does not assume a heirarchy between the classes.\n",
        "\n",
        "2.   Split the data into training and testing sets for both the feature vector and label vector.  \n",
        "\n",
        "3. Create total number variables for feature and label vectors in order to calculate random initial weights and biases.\n",
        "\n",
        "4. Create initial weights and biases as a starting point for the neural network.  We'll need a starting point for the weights and biases so that the forward propogation process can begin with these initial values.  The values will later be optimized through the backpropogation process (that is where the learning takes place).\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9A0YxtzdadJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use Sci-Kit's one-hot encoder to transform the classes into numerical values\n",
        "# to feed into our multilayer perceptron\n",
        "numVal = array(y)\n",
        "\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(numVal)\n",
        "\n",
        "# encode from integer to binary\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "# splitting up training and testing data into appropriate vectors for initial \n",
        "# group\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, onehot_encoded)\n",
        "\n",
        "# establish the number of labels and features\n",
        "num_labels = y_train.shape[1]\n",
        "num_features = X_train.shape[1]\n",
        "\n",
        "# set size of hidden layer\n",
        "hidden_nodes = 3 * len(X_train)\n",
        "\n",
        "# establish the arrays of weights and biases\n",
        "w1 = np.random.normal(0, 1, [num_features, hidden_nodes]) \n",
        "w2 = np.random.normal(0, 1, [hidden_nodes, num_labels]) \n",
        "\n",
        "b1 = np.zeros((1, hidden_nodes))\n",
        "b2 = np.zeros((1, num_labels))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BK8ovNsJcfYY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions for the neural network\n",
        "\n",
        "These functions include:\n",
        "\n",
        "\n",
        "##  *  relu_activation --> \n",
        "I have chosen the relu activation function as it has a simple implementation and is often very accurate as it solves the vanishing gradient issue with other activation functions.  It simply takes the max of 0 and the input vector x. (i.e. max(x, 0))\n",
        "\n",
        "## *   softmax -->\n",
        "The relu activation function only works when used as an activation function on the hidden layers of a neural network, therefore I have chosen to use the softmax activation function for the output layer.  The softmax function conveniently places the output values within the range [0, 1].  Since all the probabilities that will go towards the output layer will equal 1, the softmax activation function will choose the classification with the highest probability.  Easy as that!\n",
        "\n",
        "\n",
        "## *   forward -->\n",
        "The neural network will feed forward the input data.  The data will be computed at each layer using f(x) = wx + b function as w = weight values, x is input values, and b is the bias of the layer.  Then after that value is calculated at each layer, it will be put through the layer's associated activation function to keep moving forward through the neural network.\n",
        "\n",
        "\n",
        "\n",
        "## *   backprop -->\n",
        "This is the step of the process where the neural network learns, as it identifies the error of the output nodes.  This establishes a loss value and a gradient or slope for that value at each node.  When the data is fed back through the network, each value goes through gradient descent where the local minimum of its specific gradient is calculated so that it can adjust new values to the parameters of the neural networks to minimize loss the next time data is fed through the network."
      ]
    },
    {
      "metadata": {
        "id": "B0bzTg_apEpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# effective activation function of my choosing\n",
        "def relu_activation(vec):\n",
        "  return np.maximum(vec, 0)\n",
        "\n",
        "\n",
        "# returns a vector of output probabilities\n",
        "def softmax(vec):\n",
        "  # for softmax we compute input over number of choices\n",
        "  input = np.exp(vec)\n",
        "  # output is sum of all of those choices, K\n",
        "  output = np.sum(input, axis = 1, keepdims = True)\n",
        "  return input / output\n",
        "\n",
        "\n",
        "def forward(softmax_vec, onehot_labels, lam, w1, w2):\n",
        "  \n",
        "  # first we calculate softmax cross-entropy loss (refer to formula)\n",
        "  i = np.argmax(onehot_labels, axis = 1).astype(int)\n",
        "  \n",
        "  # since softmax output will be probability values (non-integer) we use function\n",
        "  # arange() \n",
        "  predicted = softmax_vec[np.arange(len(softmax_vec)), i]\n",
        "  logs = np.log(predicted)\n",
        "  loss = -np.sum(logs) / len(logs)\n",
        "  \n",
        "  # second we add regularization to the loss in order to avoid overfitting\n",
        "  w1_loss = 0.5 * lam * np.sum(w1 * w1)\n",
        "  w2_loss = 0.5 * lam * np.sum(w2 * w2)\n",
        "  return (loss + (w1_loss + w2_loss))\n",
        "  \n",
        "  \n",
        "def backprop(w1, b1, w2, b2, lam, lr, output_vec, hidden_vec):\n",
        "  output_error = (output_vec - y_train) / output_vec.shape[0]\n",
        "\n",
        "  hidden_error = np.dot(output_error, w2.T) \n",
        "  hidden_error[hidden_vec <= 0] = 0\n",
        "\n",
        "  gw2 = np.dot(hidden_vec.T, output_error)\n",
        "  gb2 = np.sum(output_error, axis = 0, keepdims = True)\n",
        "\n",
        "  gw1 = np.dot(X_train.T, hidden_error)\n",
        "  gb1 = np.sum(hidden_error, axis = 0, keepdims = True)\n",
        "\n",
        "  gw2 += lam * w2\n",
        "  gw1 += lam * w1\n",
        "\n",
        "  w1 -= lr * gw1\n",
        "  b1 -= lr * gb1\n",
        "  w2 -= lr * gw2\n",
        "  b2 -= lr * gb2\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ox1e55D9cxz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Running the network\n",
        "\n",
        "Here, we establish the amount of iterations that we will feed the data through the network in order to train it and prepare it to make predictions on new data.  epochNum in this case is the amount of iterations that we will initialize.\n",
        "***\n",
        "\n",
        "In each iteration, there are a few things that are happening:\n",
        "\n",
        "1. The values at each layer are being established with respect to that specific layer's activation function.\n",
        "\n",
        "2. The softmax activation function (in this case) will make a decision on the classificiation it believes is correct from choosing the classification with the highest probability (since the softmax outputs values between [0, 1].)\n",
        "\n",
        "3. With these established vectors at each layer, we will first feed the data forward through forward propogation to get our output values\n",
        "\n",
        "4. Then we will adjust our network's parameters in the process of backpropogation so that the network can classify more accurately in the future.\n",
        "***"
      ]
    },
    {
      "metadata": {
        "id": "XXAgC-SwczCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# since we need to return the object 'epoch' in this case we will use xrange()\n",
        "# rather than range() function in python\n",
        "\n",
        "for epoch in range(1,epochNum):\n",
        "  # wx + b\n",
        "  input = np.dot(X_train, w1) + b1\n",
        "  hidden = relu_activation(input)\n",
        "  output = np.dot(hidden, w2) + b2\n",
        "  soft_output = softmax(output)\n",
        "\n",
        "  forward(soft_output, y_train, init_lam, w1, w2)\n",
        "  backprop(w1, b1, w2, b2, init_lam, lr, output, hidden)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n1koU46AczeW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# It is time to test the network!\n",
        "\n",
        "***\n",
        "I have defined an eval() function that will put the network's prediction vector up against the actual classes of the data that it was fed.  This function pretty much just calculates the correctness by dividing the correct predictions against the total number of rows of data the network evaluated, or total number of predictions it had. \n",
        "\n",
        "***"
      ]
    },
    {
      "metadata": {
        "id": "C5yjEYMqczs5",
        "colab_type": "code",
        "outputId": "db120d18-285a-4f6b-b197-6d9aaeeff93b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# test\n",
        "\n",
        "def eval(preds, y):\n",
        "    ifcorrect =  np.argmax(preds, 1) == np.argmax(y, 1)\n",
        "    correct_predictions = np.sum(ifcorrect)\n",
        "    return correct_predictions * 100 / preds.shape[0]\n",
        "  \n",
        "\n",
        "input = np.dot(X_test, w1)\n",
        "hidden = relu_activation(input + b1)\n",
        "scores = np.dot(hidden, w2) + b2\n",
        "probs = softmax(scores)\n",
        "print('Accuracy of Multilayer Perceptron: {0}%'.format(eval(probs, y_test)))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Multilayer Perceptron: 94.73684210526316%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LOgxz1rodKoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## In Conclusion\n",
        "\n",
        "\n",
        "***\n",
        "This Multilayer Perceptron Neural Network performs at usually high levels as the majority of the time we get results of over 92%.  That means that 92% of the time, this neural network is identifying the type of flower based on just being fed various characteristics of the flower.  \n",
        "\n",
        "***\n",
        "\n",
        "I separated a code block at the top of this file where you can adjust some parameters, such as learning rate and epochNum, to see if you can find a way to get better results than I could.  \n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "These are a couple articles that I referred to that I found very helpful in providing assistance for layout and understanding.\n",
        "\n",
        "###1.[ Freecodecamp.org - Medium Article](https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3)\n",
        "\n",
        "###2. [Building a Neural Network from Scratch in Python and in TensorFlow ](https://beckernick.github.io/neural-network-scratch/)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
